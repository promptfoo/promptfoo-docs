---
sidebar_position: 1
---

# Parameters

### Prompt Files

Prompt files are plain text files that contain the prompts you want to test. If you have only one file, you can include multiple prompts in the file, separated by the delimiter `---`. If you have multiple files, each prompt should be in a separate file.

You can use [Nunjucks](https://mozilla.github.io/nunjucks/) templating syntax to include variables in your prompts, which will be replaced with actual values from the `vars` CSV file during evaluation.

Example of a single prompt file with multiple prompts (`prompts.txt`):

```
Translate the following text to French: "{{name}}: {{text}}"
---
Translate the following text to German: "{{name}}: {{text}}"
```

Example of multiple prompt files:

- `prompt1.txt`:

  ```
  Translate the following text to French: "{{name}}: {{text}}"
  ```

- `prompt2.txt`:

  ```
  Translate the following text to German: "{{name}}: {{text}}"
  ```

### Vars File

The Vars file is a CSV, JSON, or YAML file that contains the values for the variables used in the prompts. The first row of the CSV file should contain the variable names, and each subsequent row should contain the corresponding values for each test case.

Vars are substituted by [Nunjucks](https://mozilla.github.io/nunjucks/) templating syntax into prompts.

Example of a vars file (`vars.csv`):

```
"name","text"
"Bob","Hello, world!"
"Joe","Goodbye, everyone!"
```

Example of a vars file (`vars.json`):

```json
[
  { "name": "Bob", "text": "Hello, world!" },
  { "name": "Joe", "text": "Goodbye, everyone!" }
]
```

The vars file optionally supports some special columns:

- `__expected`: A column that includes [test assertions](/docs/configuration/expected-outputs).  This column lets you automatically mark output according to quality expectations.
- `__prefix`: This string is prepended to each prompt before it's sent to the API
- `__suffix`: This string is appended to each prompt before it's sent to the API

### Output File

The results of the evaluation are written to this file. Each record in the output file corresponds to a test case and includes the original prompt, the output generated by the LLM, and the values of the variables used in the test case.

For example outputs, see the [examples/](https://github.com/typpo/promptfoo/tree/main/examples) directory.

### Configuration File

You can specify any option in a configuration file (e.g., `.promptfoorc`, `promptfoo.config.json`). This can help you avoid repetitive command-line options and simplify the CLI invocation.

By default, `promptfooconfig.js` is loaded.

Example of a configuration file:

```js
module.exports = {
  "prompts": ["prompt1.txt"],
  "providers": ["openai:chat"],
  "vars": "/path/to/vars.csv",
  "maxConcurrency": 3
}
```

The configuration file uses the `EvaluateOptions` interface:

```typescript
export interface ApiProvider {
  id: () => string;
  callApi: (prompt: string) => Promise<ProviderResponse>;
}

export interface GradingConfig {
  prompt?: string;
  provider?: string | ApiProvider;
}

export interface PromptConfig {
  prefix?: string;
  suffix?: string;
  generateSuggestions?: boolean;
}

export interface EvaluateOptions {
  providers: ApiProvider[];
  prompts: string[];
  vars?: Record<string, string>;

  prompt?: PromptConfig;
  grading?: GradingConfig;

  maxConcurrency?: number;
  showProgressBar?: boolean;
}
```

#### EvaluateOptions

| Property         | Type               | Required | Description                                                                                     |
|------------------|--------------------|----------|-------------------------------------------------------------------------------------------------|
| providers        | ApiProvider[]      | Yes      | A list of API providers to be evaluated
| prompts          | string[]           | Yes      | A list of prompts to be evaluated.                                                            |
| vars             | Record<string, string>       | No       | A list of variables names and values that are substituted in the prompts |
| prompt | PromptConfig | No       | An object containing options related to the prompts.  |
| grading          | GradingConfig      | No       | An object containing options related to automatic evaluation by LLM.                                      |
| maxConcurrency   | number             | No       | Maximum number of concurrent API calls |
| showProgressBar  | boolean            | No       | Whether to show a progress bar in the console |

#### PromptOptions

| Property           | Type    | Required | Description                                         |
|--------------------|---------|----------|-----------------------------------------------------|
| prefix             | string  | No       | A prefix that is prepended to every prompt.           |
| suffix             | string  | No       | A suffix that is appended to every prompt.            |
| generateSuggestions| boolean | No       | If true, use an LLM to suggest a variation of the prompt for evaluation. |
